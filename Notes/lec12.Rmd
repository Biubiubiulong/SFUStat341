---
title: "Lecture 12"
author: "Brad McNeney"
date: '2017-04-05'
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

\scriptsize

```{r}
library(ggplot2)
library(dplyr)
library(broom)
Auto <- read.csv("Auto.csv",stringsAsFactors = FALSE)
Auto <- na.omit(Auto)
```

## Cross-validation

\small

- Reference: Chapter 5 of
An Introduction to Statistical Learning
with Applications in R
Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani

## Example data

\small

- Data on car mileage, engine size, car
weight, for 392 cars made between 1970 and 1982.

\scriptsize

```{r, fig.width=5,fig.height=2.7}
ggplot(Auto,aes(x=horsepower,y=mpg)) + geom_point()
```

## Predict `mpg` with polynomials in `horsepower`

\scriptsize

```{r}
afit <- lm(mpg ~ horsepower + I(horsepower^2) + I(horsepower^3),
           data=Auto)
tidy(afit)
```

- Suggests we need only the linear and quadratic terms.

## The validation method

\small

- Randomly split the data into a "training" set and "test" set.
- Fit the model of each polynomial degree
to  the training set and use it to 
predict observations in the test set.
- Judge the fit of each model by the mean squared error in the test set, defined 
as $MSE = \frac{1}{n_{test}} \sum_i (y_i-\hat{y}_i)^2$,
    where $n_{test}$ is the size of the test set.
- Choose the degree with the lowest MSE.

## Split the data

\small

- Several ways to do this.
    - E.G., permute the $n$ rows and take the first $n/2$
    to be the training set and the remaining $n/2$ to 
    be the test set.
    
\scriptsize

```{r}
n <- nrow(Auto)
inds <- 1:n
set.seed(42)
pAuto <- Auto[sample(inds),]
traininds <- 1:(n/2) 
trainset <- pAuto[traininds,]
testset <- pAuto[-traininds,]
```

## Split the data

\scriptsize

```{r,fig.width=5,fig.height=3}
pAuto <- mutate(pAuto, train = (inds %in% traininds))
ggplot(pAuto,aes(x=horsepower,y=mpg,color=train)) + geom_point()
```

## Validation method on the `Auto` data

\scriptsize

```{r}
validate <- function(dat,ndegrees) {
  n <- nrow(dat)
  pdat <- dat[sample(1:n),]
  traininds <- 1:(n/2) 
  trainset <- pdat[traininds,]
  testset <- pdat[-traininds,]
  MSE <- vector(length=ndegrees)
  for(degree in 1:ndegrees) {
    fit <- lm(mpg ~ poly(horsepower,degree),data=trainset)
    ptest <- predict(fit,newdata=testset)
    MSE[degree] <- mean((testset$mpg - ptest)^2)
  }
  data.frame(degree = 1:ndegrees, MSE = MSE)
}
```

## Validation method on `Auto`


\scriptsize

```{r}
validate(Auto,10)
```

## Problems with the validation method

- Splitting reduces the size of the sample used
to fit the model.
- The tuning parameter that gives the best MSE can
vary by split.

\scriptsize

```{r}
validate(Auto,10)
```

## Cross validation

\small

- Split the data into $k$ "folds"
    - Typical values of $k$ are 5 and 10
- Leave out the first fold as a test set and train on 
the remaining folds.
- Repeat, leaving out each fold in turn.
- Report the average MSE over the folds.

## Cross validation on the `Auto` data

\scriptsize

```{r}
createFolds <- function(n,k) { cut(1:n,breaks=k,labels=FALSE) }
crossValidate <- function(dat,ndegrees,k=10) {
  n <- nrow(dat)
  pdat <- dat[sample(1:n),]
  folds <- createFolds(n,k)
  MSE <- matrix(NA,nrow=ndegrees,ncol=k)
  for(degree in 1:ndegrees) {
    for(fold in 1:k) {
      trainset <- pdat[folds != fold,] 
      testset <- pdat[folds == fold,]
      fit <- lm(mpg ~ poly(horsepower,degree),data=trainset)
      ptest <- predict(fit,newdata=testset)
      MSE[degree,fold] <- mean((testset$mpg - ptest)^2)
    }
  }
  data.frame(degree = 1:ndegrees, MSE = rowMeans(MSE))
}
```

## Cross validation on `Auto`

\scriptsize

```{r}
crossValidate(Auto,ndegrees=10)
```


## The `caret` package

- The `caret` package is supposed to
contain many functions to help with
training and fitting models.
    - Have never used it myself.
